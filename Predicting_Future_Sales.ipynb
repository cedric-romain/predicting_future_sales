{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting Future Sales\n",
    "\n",
    "This notebook contains the process of the data for my submission\n",
    "to the Kaggle competition [\"Predicting Future Sales\"](https://www.kaggle.com/c/competitive-data-science-predict-future-sales).\n",
    "\n",
    "A setup guide for the runtime environment can be found in the projects\n",
    "[README](./README.adoc) file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Control notebook flow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "run_eda = False\n",
    "run_fe = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Advanced features\n",
    "from itertools import product  # make a cross product\n",
    "from datetime import datetime\n",
    "from calendar import monthrange  # get the amount of days in a given YYYY-MM\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# My packages\n",
    "import src.plotting.Defaults\n",
    "from src.datacleaning.ConvertingDateValues import date_to_month, date_to_iso8601, iso8601_format\n",
    "from src.util.FunctionExecTime import time_runtime\n",
    "from src.util.ParseDataframe import get_item_cnt_metric\n",
    "from src.util.ListActions import cl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set package options"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%matplotlib inline\n",
    "# set the default marker style for all plots\n",
    "src.plotting.Defaults.set_defaults()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "default_converters = {\n",
    "    'date_block_num': eval,\n",
    "    'shop_id': eval,\n",
    "    'item_id': eval,\n",
    "    'item_price': eval,\n",
    "    'item_cnt_day': eval,\n",
    "}\n",
    "\n",
    "primary_columns = ['date_block_num', 'shop_id', 'item_id']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if run_eda or run_fe:\n",
    "    # Metadata packages\n",
    "    items = pd.read_csv('data/info/items.csv')\n",
    "    shops = pd.read_csv('data/info/shops.csv')\n",
    "    cats = pd.read_csv('data/info/item_categories.csv')\n",
    "\n",
    "    # Train and test data\n",
    "    train = pd.read_csv('data/technical/sales_train.csv',\n",
    "                        parse_dates=['date'],\n",
    "                        dayfirst=True,\n",
    "                        converters=default_converters,\n",
    "                        )\n",
    "    test  = pd.read_csv('data/technical/test.csv',\n",
    "                        converters=default_converters,\n",
    "                        ).set_index('ID')\n",
    "\n",
    "    # adapt data size\n",
    "    train['date_block_num'] = train['date_block_num'].astype(np.int8)\n",
    "    train['shop_id'] = train['shop_id'].astype(np.int8)\n",
    "    train['item_id'] = train['item_id'].astype(np.int16)\n",
    "    train['item_price'] = train['item_price'].astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data formatting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of function \"date_to_month\": 11s\n"
     ]
    }
   ],
   "source": [
    "if run_eda or run_fe:\n",
    "    # contains the raw train with months only\n",
    "    train_in_months = train.copy()\n",
    "    train_in_months = time_runtime(date_to_month, train_in_months)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Map the date to its index for later use"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def generate_date_mapper():\n",
    "    \"\"\"\n",
    "    Map the date in YYYY-MM format to the given `date_block_num` table.\n",
    "    \"\"\"\n",
    "    date_block_num_set = set(train_in_months['date_block_num'])\n",
    "    date_set = set(train_in_months['date'])\n",
    "\n",
    "    date_block_num_distinct = list(date_block_num_set)\n",
    "    date_block_num_distinct.sort()\n",
    "    date_distinct = list(date_set)\n",
    "    date_distinct.sort()\n",
    "\n",
    "    mapper = {}\n",
    "    for i in date_block_num_distinct.__iter__():\n",
    "        mapper.update({date_block_num_distinct[i]: date_distinct[i]})\n",
    "\n",
    "    # append the month which is to be predicted\n",
    "    mapper.update({34: '2015-11'})\n",
    "\n",
    "    return mapper\n",
    "\n",
    "def get_month_from_index(i):\n",
    "    \"\"\"\n",
    "    Returns the month in 'YYYY-MM' format of a given `date_block_num` index.\n",
    "    \"\"\"\n",
    "    return date_mapper.get(i)\n",
    "\n",
    "def get_index_of_month(month):\n",
    "    \"\"\"\n",
    "    Returns the index in the `date_block_num` table of a given month\n",
    "    (in YYYY-MM format).\n",
    "    \"\"\"\n",
    "    return list(date_mapper.keys())[list(date_mapper.values()).index(month)]\n",
    "\n",
    "if run_eda or run_fe:\n",
    "    date_mapper = generate_date_mapper()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "## Exploratory Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean features\n",
    "\n",
    "First, verify that there are no incomplete entries in the dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    train.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View monthly sales"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def view_all_sales():\n",
    "    # create a new data frame with the relevant columns.\n",
    "    df = pd.DataFrame(train_in_months, columns=['date', 'item_cnt_day'])\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    # sum the values to get a monthly overview\n",
    "    df = df.groupby(['date']).sum()\n",
    "\n",
    "    # rename the column to match new output\n",
    "    df.rename(columns = {'item_cnt_day':'item_cnt_month'}, inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "if run_eda:\n",
    "    all_sales = time_runtime(view_all_sales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    all_sales.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can observe a strong seasonality with spiking end of year sales.\n",
    "\n",
    "This is particularly noteworthy as the month to predict in question is\n",
    "going to be the month of november.\n",
    "\n",
    "Additionally, the trend seems to be going downwards over time.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View monthly revenue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def compute_revenue(df):\n",
    "    item_cnt_metric = get_item_cnt_metric(df)\n",
    "    df['revenue'] = df.apply(lambda x: x[item_cnt_metric] * x['item_price'], axis=1)\n",
    "    return df\n",
    "\n",
    "def check_revenue():\n",
    "    # pick the relevant columns\n",
    "    df = pd.DataFrame(train_in_months, columns=['date', 'item_cnt_day', 'item_price'])\n",
    "    # calculate the revenue\n",
    "    df = compute_revenue(df)\n",
    "    # drop no longer needed columns\n",
    "    df.drop(['item_cnt_day', 'item_price'], axis=1, inplace=True)\n",
    "\n",
    "    # set the index\n",
    "    df.set_index('date', inplace=True)\n",
    "    # and regroup the monthly revenue\n",
    "    return df.groupby(['date']).sum()\n",
    "\n",
    "if run_eda:\n",
    "    revenue = time_runtime(check_revenue)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    revenue.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fortunately, the revenue is closely correlated to the total sales as expected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect `item_cnt_day`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    plt.figure(figsize=(10,2))\n",
    "    sns.boxplot(x=train_in_months.item_cnt_day)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two outliers can be observed. This will have to be examined further down the line."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect `item_price`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    plt.figure(figsize=(10,2))\n",
    "    sns.boxplot(x=train_in_months.item_price)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One outlier can be observed. This will have to be examined further down the line.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect `item_id`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def inspect_total_sales_per_item_cat():\n",
    "    df = pd.DataFrame(train_in_months, columns=['item_cnt_day', 'item_id'])\n",
    "    df = df.set_index('item_id', drop=True).join(items.set_index('item_id'))\n",
    "    df.drop(['item_name'], axis=1, inplace=True)\n",
    "    df.set_index(['item_category_id'], inplace=True)\n",
    "    df = df.groupby(['item_category_id']).sum()\n",
    "    df.rename(columns = {'item_cnt_day':'item_cnt_total'}, inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "if run_eda:\n",
    "    sales_per_item_cat = time_runtime(inspect_total_sales_per_item_cat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.bar(sales_per_item_cat.index, sales_per_item_cat['item_cnt_total'])\n",
    "    plt.xlabel(\"Item category ID\")\n",
    "    plt.ylabel(\"Total item category sales\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Inspect outliers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Obvious top-performers are found. Let's find out what the category corresponds\n",
    "to as well as verify that there are no categories without any sale records."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    top_performing_item_categories = sales_per_item_cat[sales_per_item_cat['item_cnt_total'] > 200_000]\n",
    "    top_performing_item_categories = top_performing_item_categories.join(cats, on='item_category_id', how='inner')\n",
    "    top_performing_item_categories.head(top_performing_item_categories.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look at the translation ([using Google Translate](https://translate.google.com/?sl=ru&tl=en&op=translate))\n",
    "\n",
    "| Russian                               | Translation                           |\n",
    "| ------------------------------------- | ------------------------------------- |\n",
    "| Игры - PS3                            | Games - PS3                           |\n",
    "| Игры PC - Стандартные издания\t        | PC Games - Standard Editions          |\n",
    "| Кино - Blu-Ray\t                    | Movie - Blu-Ray                       |\n",
    "| Кино - DVD\t                        | Movies - DVD                          |\n",
    "| Музыка - CD локального производства   | Music - locally produced CD           |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    low_performing_item_categories = sales_per_item_cat[sales_per_item_cat['item_cnt_total'] < 10]\n",
    "    low_performing_item_categories = low_performing_item_categories.join(cats, on='item_category_id', how='inner')\n",
    "    low_performing_item_categories.head(low_performing_item_categories.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look at the translation ([using Google Translate](https://translate.google.com/?sl=ru&tl=en&op=translate))\n",
    "\n",
    "\n",
    "| Russian                               | Translation                           |\n",
    "| ------------------------------------- | ------------------------------------- |\n",
    "| PC - Гарнитуры/Наушники\t            | PC - Headsets / Headphones            |\n",
    "| Аксессуары - PS2\t                    | Accessories - PS2                     |\n",
    "| Игровые консоли - PS2\t                | Game consoles - PS2                   |\n",
    "| Игры MAC - Цифра\t                    | MAC Games - Number                    |\n",
    "| Книги - Компьютерная литература\t    | Books - Computer Literature           |\n",
    "| Книги - Открытки\t                    | Books - Postcards                     |\n",
    "| Книги - Познавательная литература     | Books - Educational literature        |\n",
    "| Книги - Путеводители                  | Books - Guides                        |\n",
    "| Книги - Художественная литература     | Books - Fiction                       |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Books seem to be in very low demand. Let's find out if there are any other categories containing books."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    other_book_cats = cats[cats['item_category_name'].str.contains(\"Книги\")]\n",
    "    other_book_cats.head(other_book_cats.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    book_sales = sales_per_item_cat[sales_per_item_cat.index >= 42]\n",
    "    book_sales = book_sales[book_sales.index <= 54]\n",
    "    book_sales.head(book_sales.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect the average amount of books sold, which is expected to be well\n",
    "below average.\n",
    "\n",
    "The most selling book subcategories within the book section are namely\n",
    "category 43 with \"Audiobooks\" (Книги - Аудиокниги) and category 49\n",
    "with \"methodical materials\" from the in-store brand from [1C](https://1c.ru/eng/title.htm).\n",
    "(Книги - Методические материалы 1С)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# average amount of sold books per category\n",
    "if run_eda:\n",
    "    int(book_sales['item_cnt_total'].sum() / book_sales.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Inspect overall average of sales of each item category"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# overall average of sold items per item category\n",
    "if run_eda:\n",
    "    int(sales_per_item_cat['item_cnt_total'].sum() / sales_per_item_cat.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conclusion\n",
    "\n",
    "We can see that for this \"software store\", media seems to be of particularly popular demand.\n",
    "\n",
    "The categories containing various subcategories of books is well below the average demand in a store.\n",
    "\n",
    "Potential features could be extracted from this information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect `shop_id`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def inspect_total_sales_per_shop():\n",
    "    df = pd.DataFrame(train_in_months, columns=['item_cnt_day', 'shop_id'])\n",
    "    df.set_index(['shop_id'], inplace=True)\n",
    "    df.rename(columns = {'item_cnt_day':'item_cnt_total'}, inplace = True)\n",
    "\n",
    "    return df.groupby(['shop_id']).sum()\n",
    "\n",
    "if run_eda:\n",
    "    sales_per_shop = time_runtime(inspect_total_sales_per_shop)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.bar(sales_per_shop.index, sales_per_shop['item_cnt_total'])\n",
    "    plt.xlabel(\"Shop ID\")\n",
    "    plt.ylabel(\"Total shop sales\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Inspect outliers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shop Nr. 36 appears to be non-existent. Let's verify that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    int(sales_per_shop.iloc[36]['item_cnt_total'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Inspect overall average of sales per shop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    int(sales_per_shop['item_cnt_total'].sum() / sales_per_shop.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conclusion\n",
    "\n",
    "We verified that each and every shop has a positive sales record.\n",
    "\n",
    "The average value of 60k sales per shop in total can be confirmed in the bar diagram showing the distribution:\n",
    "there are about as many shop's with under-average performance as there are shops with an exceptionally high record.\n",
    "\n",
    "The location of the shops might be of additional interest.\n",
    "\n",
    "Shop Nr. 55 is the Online Store of the company.\n",
    "\n",
    "Shop Nr. 20 to 32 are all located in the capital city of Moscow\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect the time series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    train_in_iso8601 = train.copy()\n",
    "    train_in_iso8601 = time_runtime(date_to_iso8601, train_in_iso8601)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    all_dates = set()\n",
    "\n",
    "def put_all_days_in_set():\n",
    "    for index, row in train_in_iso8601.iterrows():\n",
    "        date = row['date']  # access the row\n",
    "        date = datetime.strptime(date, '%Y-%m-%d')  # convert to datetime format\n",
    "        date_as_int = date.toordinal()  # convert to ordinal format for date arithmetic\n",
    "        all_dates.add((date_as_int, date))  # add to set\n",
    "\n",
    "if run_eda:\n",
    "    time_runtime(put_all_days_in_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# transform the set to a list and sort it\n",
    "if run_eda:\n",
    "    all_dates = list(all_dates)\n",
    "    all_dates.sort()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    counter_entries_in_all_dates = len(all_dates)\n",
    "    anomalies_in_date_series = set()\n",
    "\n",
    "def check_for_interruptions():\n",
    "    for i in range(counter_entries_in_all_dates):\n",
    "\n",
    "\n",
    "        if i != (counter_entries_in_all_dates - 1):\n",
    "            ordinal_this_date = all_dates[i][0]\n",
    "            ordinal_successor_date = all_dates[i+1][0]\n",
    "\n",
    "            if ordinal_this_date + 1 != ordinal_successor_date:\n",
    "                print(f\"{i}, {all_dates[i]}\")\n",
    "                anomalies_in_date_series.add(i)\n",
    "\n",
    "if run_eda:\n",
    "    time_runtime(check_for_interruptions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    all_dates[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No breaks in between dates of sale made.\n",
    "\n",
    "The last record is indeed on the last day of October."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect the correlation between features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def draw_correlation_matrix():\n",
    "    corr = train.corr()\n",
    "\n",
    "    # Generate a mask to hide duplicate values\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        corr,\n",
    "        mask=mask,\n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        cmap=sns.diverging_palette(20, 220, n=200),\n",
    "        square=True\n",
    "    )\n",
    "    # format the axis output\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=45,\n",
    "        horizontalalignment='right'\n",
    "    );  # the ';' suppresses the output\n",
    "\n",
    "\n",
    "if run_eda:\n",
    "    draw_correlation_matrix()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observe no strong correlation between the features at this point.\n",
    "The only slight hint lies between the correlation between the `item_price` and the date of the sale in `date_block_num`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results of the in-depth EDA.\n",
    "\n",
    "Final observations from the [in-depth EDA](./In_Depth_EDA.ipynb)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following datapoints from an item categories and shops perspective are indicating an unpredictable outcome that needs to be accounted for in the models.\n",
    "\n",
    "*__Item categories:__*\n",
    "\n",
    "The following categories have a mere one to two entries:\n",
    "\n",
    "1, 10, 50, 51, 52\n",
    "\n",
    "The following categories suggest very unpredictable results:\n",
    "\n",
    "0, 18, 27, 36, 48, 53, 80\n",
    "\n",
    "The following categories show a clear downward trend:\n",
    "\n",
    "13, 21, 30, 40, 43, 45, 59, 62, 66, 77, 81, 82\n",
    "\n",
    "\n",
    "*__Shops:__*\n",
    "\n",
    "The following shops have a mere one to two entries:\n",
    "\n",
    "0, 1, 11, 20, 36\n",
    "\n",
    "The following shops suggest very unpredictable results:\n",
    "\n",
    "8, 9, 23, 32, 33\n",
    "\n",
    "The decline in sales per shop is not as striking as the one on a category basis.\n",
    "This indicates that there are items going out of fashion while the shops are able to secure their relevancy in the market with an adapting supply."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect the submission set\n",
    "\n",
    "The template for which features are to be predicted is included with the\n",
    "challenge. As the dataset is highly volatile with selected shops having no sales\n",
    "in given categories on a given month.\n",
    "Let's analyse which items are relevant and included in previous data. We suspect\n",
    "that there could be new items in the submission which have no record in\n",
    "previous data at all."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find newly appearing items"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    test_item_id_set = set(test.item_id)\n",
    "    train_item_id_set = set(train.item_id)\n",
    "    item_id_sets_intersection = test_item_id_set - test_item_id_set.intersection(train_item_id_set)\n",
    "    len(item_id_sets_intersection)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Inspect the total of distinct test items"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    count_distinct_items = len(test_item_id_set)\n",
    "    count_distinct_items"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Inspect the total size of the test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    test_length = len(test)\n",
    "    test_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find relation between items and shops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    test_length / count_distinct_items"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Approximate the existing coverage of entries\n",
    "\n",
    "With having 5100 items from 42 shops inspected, while having a record of 33\n",
    "months of data present. We can estimate the coverage of existing entries for\n",
    "a given datapoint."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    train_length = len(train)\n",
    "    coverage = train_length / (test_length * 33)\n",
    "    coverage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, there are a total of 5100 distinct items in the test set\n",
    "which is to be predicted. The total size of the test data is 214.200, the\n",
    "exact size of the submission sample.\n",
    "\n",
    "The relationship between the total length and the distinct items conducts\n",
    "that exactly 42 distinct shops are going to be predicted.\n",
    "\n",
    "363 items are new in the test set which have\n",
    "no record in any of the train data.\n",
    "\n",
    "The newly added data will have to be accounted for and added to the train\n",
    "set by adding datapoints which indicate the model that this item has not\n",
    "been sold before.\n",
    "\n",
    "Additionally, we observe that the coverage of dedicated entry point for a\n",
    "given item is at best 42%. The high volatility and constantly changing\n",
    "product range indicates that it is likely even less. It will be a good idea\n",
    "to fill the train set with explicit zero sale entries for the items.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "## Feature engineering\n",
    "\n",
    "Begin by allocating a final dataframe where all the new features are going to\n",
    "be stored."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "if run_fe:\n",
    "    feature_engineered = train_in_months.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before proceeding, store two hashmaps which contain the last months of sale\n",
    "from a given item or item + shop combination.\n",
    "This is done now to parse the data (~3 million combinations) before\n",
    "inflating it with zero-sales later on (~10 million combination)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of function \"parse_table_and_store_most_recent_sale\": 2min & 37s\n"
     ]
    }
   ],
   "source": [
    "if run_fe:\n",
    "    all_items_last_sale = {}\n",
    "    all_items_first_sale = {}\n",
    "    all_items_last_sale_per_shop = {}\n",
    "    all_items_first_sale_per_shop = {}\n",
    "\n",
    "def parse_table_and_store_most_recent_sale(df):\n",
    "\n",
    "    global all_items_last_sale, all_items_last_sale_per_shop\n",
    "    global all_items_first_sale, all_items_first_sale_per_shop\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # assign the keys for both dicts\n",
    "        key_item = row.item_id\n",
    "        key_pair = (row.item_id, row.shop_id)\n",
    "\n",
    "        # if the key was not present yet, add it\n",
    "        if key_pair not in all_items_last_sale_per_shop:\n",
    "            all_items_first_sale_per_shop[key_pair] = row.date_block_num\n",
    "            all_items_last_sale_per_shop[key_pair] = row.date_block_num\n",
    "        # else, verify that the sale is indeed more recent\n",
    "        else:\n",
    "            if row.date_block_num >= all_items_last_sale_per_shop[key_pair]:\n",
    "                # and add it if successful\n",
    "                all_items_last_sale_per_shop[key_pair] = row.date_block_num\n",
    "            else:\n",
    "                all_items_first_sale_per_shop[key_pair] = row.date_block_num\n",
    "\n",
    "\n",
    "        # do it again for the isolated item\n",
    "        if key_item not in all_items_last_sale:\n",
    "            all_items_first_sale[key_item] = row.date_block_num\n",
    "            all_items_last_sale[key_item] = row.date_block_num\n",
    "        else:\n",
    "            if row.date_block_num >= all_items_last_sale[key_item]:\n",
    "                all_items_last_sale[key_item] = row.date_block_num\n",
    "            else:\n",
    "                all_items_first_sale[key_item] = row.date_block_num\n",
    "\n",
    "if run_fe:\n",
    "    time_runtime(parse_table_and_store_most_recent_sale, feature_engineered)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Append the test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def append_test_data(df):\n",
    "    df_test = test.copy()\n",
    "    df_test['date'] = '2015-11'\n",
    "    df_test['date_block_num'] = 34\n",
    "    return df.append(df_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batch feature engineering\n",
    "\n",
    "Create a function which makes sure to apply the same operations on the train\n",
    "and testing dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def apply_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Calls all functions to apply the feature engineering equally on the\n",
    "    train and test dataset.\n",
    "    \"\"\"\n",
    "    df = time_runtime(regroup_monthly_sales, df)\n",
    "    df = time_runtime(apply_cat_info, df)\n",
    "    df = time_runtime(apply_shop_duplicates, df)\n",
    "    df = time_runtime(apply_shop_location, df)\n",
    "    df = time_runtime(encode_categorical_data, df)\n",
    "\n",
    "    df = time_runtime(add_item_cat_avg_price, df)\n",
    "    df = time_runtime(apply_item_cat_avg_price_to_new_items, df)\n",
    "    df = time_runtime(apply_revenue_feature, df)\n",
    "    df = time_runtime(fillna_revenue_new_items, df)\n",
    "\n",
    "    df = time_runtime(add_month_feature, df)\n",
    "    df = time_runtime(add_days_in_month, df)\n",
    "    df = time_runtime(add_item_last_sale_in_shop, df)\n",
    "    df = time_runtime(add_item_last_sale, df)\n",
    "    df = time_runtime(add_item_first_sale_in_shop, df)\n",
    "    df = time_runtime(add_item_first_sale, df)\n",
    "\n",
    "\n",
    "    df = time_runtime(create_zero_sales, df)\n",
    "    df = time_runtime(fill_missing_data, df)\n",
    "\n",
    "    df = time_runtime(apply_label_lag_feature, df)\n",
    "    df = time_runtime(add_detailed_lag_features, df)\n",
    "    df = time_runtime(add_average_price, df)\n",
    "    df = time_runtime(add_average_price_during_month, df)\n",
    "    df = time_runtime(add_lag_to_avg_price_during_month, df)\n",
    "    df = time_runtime(add_price_trend, df)\n",
    "    df = time_runtime(add_shop_revenue_per_month, df)\n",
    "    df = time_runtime(add_shop_avg_revenue, df)\n",
    "    df = time_runtime(add_shop_avg_revenue_trend, df)\n",
    "    df = time_runtime(add_shop_avg_revenue_trend_lag, df)\n",
    "\n",
    "    df = time_runtime(fillna_lag_features, df)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a static list to store the categorical columns to encode these later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "if run_fe:\n",
    "    encode_categorical_features = set()\n",
    "    track_all_lag_features = set()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert the daily sales record to a monthly rhythm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def regroup_monthly_sales(df):\n",
    "\n",
    "    cols = ['date_block_num','shop_id','item_id']\n",
    "\n",
    "    df_item_price = df.groupby(cols).agg({'item_price': ['mean']})\n",
    "    df = df.groupby(cols).agg({'item_cnt_day': ['sum']})\n",
    "\n",
    "    df = pd.merge(df, df_item_price, how=\"inner\", on=cols)\n",
    "\n",
    "    df.rename(columns = {'item_cnt_day':'item_cnt_month'}, inplace = True)\n",
    "    # clip it according to the description of the competition\n",
    "    df['item_cnt_month'] = df['item_cnt_month'].clip(0, 20)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    df = add_dateblocknum_to_date(df)\n",
    "\n",
    "    # remove the multi-index create by the `agg` function\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    df['item_cnt_month'] = df['item_cnt_month'].astype(np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_dateblocknum_to_date(df):\n",
    "    df['date'] = df['date_block_num'].apply(lambda x: get_month_from_index(x))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add information about the categories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def add_item_category_id(df):\n",
    "    df = pd.merge(df, items, on='item_id')\n",
    "    df.drop(['item_name'], axis=1, inplace=True)\n",
    "    df['item_category_id'] = df['item_category_id'].astype(np.int8)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def get_category_info(df):\n",
    "    global encode_categorical_features\n",
    "    \"\"\"\n",
    "    Return the dataframe containing all the additional information about the\n",
    "    item categories. Read the CSV, drop the irrelevant tables and join on the\n",
    "    item_category_id.\n",
    "    \"\"\"\n",
    "    cat_info = pd.read_csv(\"data/feature_engineering/item_categories_classified.csv\")\n",
    "    cat_info.drop(['item_category_name', 'english_name'], axis=1, inplace=True)\n",
    "\n",
    "    encode_categorical_features.add('category_type')\n",
    "    encode_categorical_features.add('category_device')\n",
    "    encode_categorical_features.add('category_device_for_platform')\n",
    "    encode_categorical_features.add('category_device_for_platform_manufacturer')\n",
    "    encode_categorical_features.add('category_medium_type')\n",
    "\n",
    "    df = pd.merge(df, cat_info, on='item_category_id')\n",
    "\n",
    "    df['category_is_fancy'] = df['category_is_fancy'].astype(bool)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def apply_cat_info(df):\n",
    "    df = add_item_category_id(df)\n",
    "    df = get_category_info(df)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encode categorical data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def encode_categorical_data(df):\n",
    "    global encode_categorical_features\n",
    "\n",
    "    for cat_feat in encode_categorical_features:\n",
    "        df[f'{cat_feat}'] = LabelEncoder().fit_transform(df[cat_feat])\n",
    "        df[f'{cat_feat}'] = df[f'{cat_feat}'].astype(np.int8)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fill the train data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Add the missing data\n",
    "\n",
    "Populate the train data with non-existing sales to indicate that the item\n",
    "is accounted for but has not been sold in a given month."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def merge_remaining_data_back(df, df_cp, cols):\n",
    "    \"\"\"\n",
    "    Merge the cross-product dataframe with zero-sales and drop the duplicated\n",
    "    values at the end.\n",
    "    \"\"\"\n",
    "    df = pd.merge(df, df_cp, on=cols, how=\"outer\")\n",
    "    return df.drop_duplicates(cols)\n",
    "\n",
    "\n",
    "def create_zero_sales(df):\n",
    "    \"\"\"\n",
    "    Create a cross-product between the date_block_num, unique shop_id's and\n",
    "    unique item_id's. This will allow us to indicate sales that have never\n",
    "    happened to allow for a more even distribution of the sales.\n",
    "    \"\"\"\n",
    "    df_cross_product = []\n",
    "    relevant_cols = ['date_block_num', 'shop_id', 'item_id']\n",
    "    for i in range(34):\n",
    "        sales = df[df.date_block_num == i]\n",
    "        df_cross_product.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())),\n",
    "                                         dtype='int16'))\n",
    "\n",
    "    df_cross_product = pd.DataFrame(np.vstack(df_cross_product), columns=relevant_cols)\n",
    "    df_cross_product['date_block_num'] = df_cross_product['date_block_num'].astype(np.int8)\n",
    "    df_cross_product['shop_id'] = df_cross_product['shop_id'].astype(np.int8)\n",
    "    df_cross_product['item_id'] = df_cross_product['item_id'].astype(np.int16)\n",
    "    df_cross_product.sort_values(relevant_cols,inplace=True)\n",
    "\n",
    "    df_cross_product = merge_remaining_data_back(df, df_cross_product, relevant_cols)\n",
    "\n",
    "    return df_cross_product"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fill the missing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def fill_with_itemprice_mean(df):\n",
    "\n",
    "    df_item_price_avg = df.groupby('item_id').agg({'item_price': ['mean']})\n",
    "    df_item_price_avg.columns = df_item_price_avg.columns.droplevel(1)\n",
    "\n",
    "    # transform to hashmap to make quicker searches\n",
    "    price_avg_dict = df_item_price_avg.to_dict()\n",
    "    price_avg_dict = price_avg_dict.get('item_price')\n",
    "\n",
    "    df['item_price'] = df.apply(\n",
    "        lambda row: price_avg_dict.get(row['item_id'])\n",
    "        if np.isnan(row['item_price']) else row['item_price'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def fill_zero_sales(df):\n",
    "    df['item_cnt_month'] = df['item_cnt_month'].fillna(0)\n",
    "    return df\n",
    "\n",
    "def fill_missing_data(df):\n",
    "    df = add_dateblocknum_to_date(df)\n",
    "    df = fill_zero_sales(df)\n",
    "    df = fill_with_itemprice_mean(df)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Duplicated shops\n",
    "\n",
    "When inspecting their names, A few shops appear to be duplicates of each other.\n",
    "Namely:\n",
    "\n",
    "| Shop name                         | Shop ID | Shop name                   | Shop ID |\n",
    "| -------------------------         | ------- | --------------------------- | ------- |\n",
    "| !Якутск Орджоникидзе, 56 фран     | 0       | Якутск Орджоникидзе, 56     | 57      |\n",
    "| !Якутск ТЦ \"Центральный\" фран     | 1       | Якутск ТЦ \"Центральный\"     | 58      |\n",
    "| Жуковский ул. Чкалова 39м?        | 10      | Жуковский ул. Чкалова 39м²  | 11      |\n",
    "\n",
    "\"фран\" means \"fran\". Most likely meaning \"franchise\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fortunately, this eliminates 3 of the 5 shops with few data entries from the shop list. (0, 1 and 11)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def apply_shop_duplicates(df):\n",
    "    \"\"\"\n",
    "    Fix the duplicated shops from the data. Pass a dataframe (train/test)\n",
    "    to apply the fix to all the entries.\n",
    "    \"\"\"\n",
    "    df.loc[df.shop_id == 0, 'shop_id'] = 57\n",
    "    df.loc[df.shop_id == 1, 'shop_id'] = 58\n",
    "    df.loc[df.shop_id == 11, 'shop_id'] = 10\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Shop location\n",
    "\n",
    "Preparation: craft a new feature from examining the locations in Russia\n",
    "from all stores.\n",
    "First, put all the cities into a dedicated dataframe to closely inspect all\n",
    "distinct locations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "if run_fe:\n",
    "    shops_cities_distribution = pd.DataFrame()\n",
    "    shops_cities_distribution['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "    # the earlier found cities that are prepended with an exclamation mark are removed:\n",
    "    shops_cities_distribution.loc[shops_cities_distribution.city == '!Якутск', 'city'] = 'Якутск'\n",
    "\n",
    "    # drop the duplicated cities and get a table containing all 31 cities\n",
    "    shops_cities_distribution.drop_duplicates(inplace=True)\n",
    "    shops_cities_distribution.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare the data containing more detailed information about the physical\n",
    "location of the shops.\n",
    "As the location match several shops, the dataset contains a list of all the\n",
    "shops where they apply to and therefore need to be assigned new rows to be\n",
    "able to merge it later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def get_shop_info():\n",
    "    \"\"\"\n",
    "    Return the dataframe containing all the additional information about the\n",
    "    single shops. Read the CSV, drop the irrelevant tables and parse the shop_id\n",
    "    that are stored as String into a dedicated list.\n",
    "    \"\"\"\n",
    "\n",
    "    shop_info = pd.read_csv(\"data/feature_engineering/cities.csv\",\n",
    "                            converters={\n",
    "                                'zip_code': eval,\n",
    "                                'population': eval,\n",
    "                                'population_growth': eval,\n",
    "                                'region_gdp_in_usd': eval,\n",
    "                            }\n",
    "                            )\n",
    "    # drop additional information\n",
    "    shop_info.drop(['wiki', 'additional_links', 'gdp_source', 'city', 'city_in_english'], axis=1, inplace=True)\n",
    "    # store the column names for later use\n",
    "    shop_info_indices = list(shop_info.columns)\n",
    "    # transform the shop_id CSV string to list\n",
    "    shop_info['shop_ids'] = shop_info['shop_ids'].transform(lambda x: x.split(\";\"))\n",
    "\n",
    "    # assign every entry in shop_ids into their own row\n",
    "    for _, row in shop_info.iterrows():  # for every row\n",
    "        new_row = {}  # create a new dict to store new values\n",
    "        for shop_id in row['shop_ids']:  # for every shop_id in shop_ids\n",
    "            new_row['shop_id'] = int(shop_id)  # assign the found shop_id\n",
    "            for key in shop_info_indices:  # add the old key/value pairs\n",
    "                if key != \"shop_ids\":  # ignore the shop_ids\n",
    "                    new_row[key] = row[key]  # assign the key/value\n",
    "            shop_info = shop_info.append(new_row, ignore_index=True)  # append the new dict\n",
    "        shop_info.drop(index=0, inplace=True)  # drop the old entry which is always the first entry\n",
    "\n",
    "    shop_info.drop([\"shop_ids\"], axis=1, inplace=True)  # drop the no longer needed column\n",
    "\n",
    "    encode_categorical_features.add('zip_code')\n",
    "    encode_categorical_features.add('region')\n",
    "    return shop_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merge the location data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def apply_shop_location(df):\n",
    "    global encode_categorical_features\n",
    "    shop_info = get_shop_info()\n",
    "    df =pd.merge(df, shop_info, on=\"shop_id\", how=\"inner\")\n",
    "    df['zip_code'] = df['zip_code'].astype(np.int32)\n",
    "    df['population'] = df['population'].astype(np.int32)\n",
    "    df['population_growth'] = df['population_growth'].astype(np.float16)\n",
    "    df['region_gdp_in_usd'] = df['region_gdp_in_usd'].astype(np.int32)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add the revenue feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def apply_revenue_feature(df):\n",
    "    return compute_revenue(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add lag features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def compute_lag_feature(df, lags, feature):\n",
    "    global track_all_lag_features\n",
    "    tmp = df[cl(primary_columns, feature)]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        new_col_name = f\"{feature}_lag_{str(i)}\"\n",
    "        shifted.columns = cl(primary_columns, new_col_name)\n",
    "        # shift to the right according to the index\n",
    "        shifted['date_block_num'] += i\n",
    "        # merge on the primary columns\n",
    "        df = pd.merge(df, shifted, on=primary_columns, how='left')\n",
    "        track_all_lag_features.add(new_col_name)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Start with the label\n",
    "\n",
    "We are going to lag the target label. We are going to check how many times the\n",
    "item has been sold `i` months prior."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def apply_label_lag_feature(df):\n",
    "    lags = [1, 2, 3, 6, 12]\n",
    "    feature = 'item_cnt_month'\n",
    "    df = compute_lag_feature(df, lags, feature)\n",
    "\n",
    "    for lag in lags:\n",
    "        new_col_name = f\"{feature}_lag_{str(lag)}\"\n",
    "        df[new_col_name] = df[new_col_name].astype(np.float16)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Add more detailed lag features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def pass_lag_features(df, group, target):\n",
    "    # calculate the mean of the targeted group\n",
    "    group_with_mean = df.groupby(group).agg({'item_cnt_month': ['mean']})\n",
    "    # create the new target column for the lag feature to be stored to\n",
    "    group_with_mean.columns = [target]\n",
    "    # reset the index\n",
    "    group_with_mean.reset_index(inplace=True)\n",
    "\n",
    "    # merge the new data\n",
    "    df = pd.merge(df, group_with_mean, on=group, how='left')\n",
    "    # define the column type\n",
    "    df[target] = df[target].astype(np.float16)\n",
    "    # calculate the lag feature with the previously computed mean value\n",
    "    df = compute_lag_feature(df, [1, 2, 3, 6, 12], target)\n",
    "    # drop the mean column as it has been added as lag feature\n",
    "    df.drop([target], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_detailed_lag_features(df):\n",
    "    df = pass_lag_features(df, ['date_block_num'],\n",
    "                                'date_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'item_id'],\n",
    "                                'date_item_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'shop_id'],\n",
    "                                'date_shop_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'zip_code'],\n",
    "                                'date_city_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'item_id', 'zip_code'],\n",
    "                                'date_item_city_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'item_category_id'],\n",
    "                           'date_cat_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'shop_id', 'item_category_id'],\n",
    "                           'date_shop_cat_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'shop_id', 'category_type'],\n",
    "                           'date_shop_cat-type_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'shop_id', 'category_device'],\n",
    "                           'date_shop_cat-device_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'category_type'],\n",
    "                           'date_cat-type_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'category_device'],\n",
    "                           'date_cat-device_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'category_medium_type'],\n",
    "                           'date_cat-medium-type_avg_item_cnt')\n",
    "    df = pass_lag_features(df, ['date_block_num', 'shop_id', 'category_medium_type'],\n",
    "                           'date_shop_cat-medium-type_avg_item_cnt')\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add trend feature for item prices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute average price"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def add_average_price(df):\n",
    "    target_col = 'item_avg_price'\n",
    "    group_on = ['item_id']\n",
    "    avg_price_grp = df.groupby(group_on).agg({'item_price': ['mean']})\n",
    "    avg_price_grp.columns = [target_col]\n",
    "    avg_price_grp.reset_index(inplace=True)\n",
    "    avg_price_grp[target_col] = avg_price_grp[target_col].astype(np.float32)\n",
    "\n",
    "    df = pd.merge(df, avg_price_grp, on=group_on, how='left')\n",
    "    df[target_col] = df[target_col].astype(np.float16)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute average price on a given month"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def add_average_price_during_month(df):\n",
    "    target_col = 'item_avg_price_during_month'\n",
    "    group_on = ['date_block_num', 'item_id']\n",
    "    avg_price_grp = df.groupby(group_on).agg({'item_price': ['mean']})\n",
    "    avg_price_grp.columns = [target_col]\n",
    "    avg_price_grp.reset_index(inplace=True)\n",
    "    avg_price_grp[target_col] = avg_price_grp[target_col].astype(np.float32)\n",
    "\n",
    "\n",
    "    df = pd.merge(df, avg_price_grp, on=group_on, how='left')\n",
    "    df[target_col] = df[target_col].astype(np.float16)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute price lag"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "if run_fe:\n",
    "    price_trend_lag = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "def add_lag_to_avg_price_during_month(df):\n",
    "    return compute_lag_feature(df, price_trend_lag, 'item_avg_price_during_month')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute price lag evolution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def add_price_trend(df):\n",
    "    avg_p = 'item_avg_price'\n",
    "    for lag in price_trend_lag:\n",
    "        target_col = f'delta_price_trend_with_lag_{lag}'\n",
    "        avg_p_lag = f'item_avg_price_during_month_lag_{lag}'\n",
    "        df[target_col] = (df[avg_p_lag] - df[avg_p]) / df[avg_p]\n",
    "        track_all_lag_features.add(target_col)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add trend feature for shop revenue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def add_shop_revenue_per_month(df):\n",
    "    target_col = 'shop_revenue_this_month'\n",
    "    group_on = ['date_block_num', 'shop_id']\n",
    "    avg_price_grp = df.groupby(group_on).agg({'revenue': ['sum']})\n",
    "    avg_price_grp.columns = [target_col]\n",
    "    avg_price_grp.reset_index(inplace=True)\n",
    "\n",
    "    df = pd.merge(df, avg_price_grp, on=group_on, how='left')\n",
    "    df[target_col] = df[target_col].astype(np.float32)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def add_shop_avg_revenue(df):\n",
    "    target_col = 'shop_avg_revenue'\n",
    "    group_on = ['shop_id']\n",
    "    avg_price_grp = df.groupby(group_on).agg({'shop_revenue_this_month': ['mean']})\n",
    "    avg_price_grp.columns = [target_col]\n",
    "    avg_price_grp.reset_index(inplace=True)\n",
    "\n",
    "    df = pd.merge(df, avg_price_grp, on=group_on, how='left')\n",
    "    df[target_col] = df[target_col].astype(np.float32)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def add_shop_avg_revenue_trend(df):\n",
    "    target_col = f'delta_shop_revenue_this_month'\n",
    "    monthly_value = 'shop_revenue_this_month'\n",
    "    avg_revenue = 'shop_avg_revenue'\n",
    "\n",
    "    df[target_col] = (df[monthly_value] - df[avg_revenue]) / df[avg_revenue]\n",
    "    df[target_col] = df[target_col].astype(np.float16)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def add_shop_avg_revenue_trend_lag(df):\n",
    "    return compute_lag_feature(df, [1, 2, 3], 'delta_shop_revenue_this_month')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add feature with information about the most recent sales"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def add_item_last_sale_in_shop(df):\n",
    "    new_col = 'item_last_sold_in_given_shop'\n",
    "    df[new_col] = -1\n",
    "    df[new_col] = df[new_col].astype(np.int8)\n",
    "    df[new_col] = df.apply(\n",
    "        lambda x: all_items_last_sale_per_shop.get( (x['item_id'], x['shop_id']) , -1),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df[new_col] = df[new_col].astype(np.int8)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def add_item_last_sale(df):\n",
    "    new_col = 'item_last_sold'\n",
    "    df[new_col] = -1\n",
    "    df[new_col] = df[new_col].astype(np.int8)\n",
    "    df[new_col] = df.apply(\n",
    "        lambda x: all_items_last_sale.get( (x['item_id']) , -1),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df[new_col] = df[new_col].astype(np.int8)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def add_item_first_sale_in_shop(df):\n",
    "    new_col = 'item_first_sold_in_given_shop'\n",
    "    df[new_col] = -1\n",
    "    df[new_col] = df[new_col].astype(np.int8)\n",
    "    df[new_col] = df.apply(\n",
    "        lambda x: all_items_first_sale_per_shop.get( (x['item_id'], x['shop_id']) , -1),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df[new_col] = df[new_col].astype(np.int8)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def add_item_first_sale(df):\n",
    "    new_col = 'item_first_sold'\n",
    "    df[new_col] = -1\n",
    "    df[new_col] = df[new_col].astype(np.int8)\n",
    "    df[new_col] = df.apply(\n",
    "        lambda x: all_items_first_sale.get( (x['item_id']) , -1),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df[new_col] = df[new_col].astype(np.int8)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add feature about month metadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def add_month_feature(df):\n",
    "    df['month'] = df.apply(lambda x: int(x['date'].split(\"-\")[1]), axis=1)\n",
    "    df['month'] = df['month'].astype(np.int8)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def add_days_in_month(df):\n",
    "    days_in_month = {}\n",
    "    for _, yyyy_mm in date_mapper.items():\n",
    "        split = yyyy_mm.split(\"-\")\n",
    "        year = int(split[0])\n",
    "        month = int(split[1])\n",
    "        days_in_month.update({yyyy_mm: monthrange(year, month)[1]})\n",
    "\n",
    "    df['days_in_month'] = df.apply(lambda x: days_in_month.get(x['date']), axis=1)\n",
    "    df['days_in_month'] = df['days_in_month'].astype(np.int8)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fill NaN for the lag features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def fillna_lag_features(df):\n",
    "    for col in track_all_lag_features:\n",
    "        df[col].fillna(0, inplace=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute item category average price"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def add_item_cat_avg_price(df):\n",
    "    target_col = 'item_cat_avg_price'\n",
    "    group_on = ['item_category_id']\n",
    "    avg_price_grp = df.groupby(group_on).agg({'item_price': ['mean']})\n",
    "    avg_price_grp.columns = [target_col]\n",
    "    avg_price_grp.reset_index(inplace=True)\n",
    "\n",
    "    df = pd.merge(df, avg_price_grp, on=group_on, how='left')\n",
    "    df[target_col] = df[target_col].astype(np.float32)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def apply_item_cat_avg_price_to_new_items(df):\n",
    "    df['item_price'] = df.apply(\n",
    "        lambda row: row['item_cat_avg_price']\n",
    "        if np.isnan(row['item_price']) else row['item_price'],\n",
    "        axis=1\n",
    "    )\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def fillna_revenue_new_items(df):\n",
    "    df['revenue'].fillna(0, inplace=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def drop_month_col(df):\n",
    "    return df.drop(['date'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def fix_col_data_types(df):\n",
    "    df['date_block_num'] = df['date_block_num'].astype(np.int8)\n",
    "    df['shop_id'] = df['shop_id'].astype(np.int8)\n",
    "    df['item_id'] = df['item_id'].astype(np.int16)\n",
    "    df['item_price'] = df['item_price'].astype(np.float32)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply the feature engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# feature_engineered = apply_feature_engineering(feature_engineered)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of function \"regroup_monthly_sales\": 1s\n",
      "Runtime of function \"create_zero_sales\": 14s\n",
      "Runtime of function \"fill_missing_data\": 1min & 28s\n"
     ]
    }
   ],
   "source": [
    "if run_fe:\n",
    "    feature_engineered = time_runtime(regroup_monthly_sales, feature_engineered)\n",
    "\n",
    "    feature_engineered = time_runtime(create_zero_sales, feature_engineered)\n",
    "    feature_engineered = time_runtime(fill_missing_data, feature_engineered)\n",
    "\n",
    "    feature_engineered = feature_engineered.sort_values(['date_block_num', 'shop_id', 'item_id'])\n",
    "    feature_engineered = append_test_data(feature_engineered)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "# storing the original index to counteract this bug\n",
    "# https://github.com/pandas-dev/pandas/issues/18776\n",
    "if run_fe:\n",
    "    feature_engineered = feature_engineered.reset_index()\n",
    "    feature_engineered['original_index'] = feature_engineered.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of function \"apply_cat_info\": 5s\n",
      "Runtime of function \"apply_shop_duplicates\": 190ms\n",
      "Runtime of function \"apply_shop_location\": 3s\n",
      "Runtime of function \"encode_categorical_data\": 10s\n",
      "Runtime of function \"add_item_cat_avg_price\": 2s\n",
      "Runtime of function \"apply_item_cat_avg_price_to_new_items\": 1min & 29s\n",
      "Runtime of function \"apply_revenue_feature\": 1min & 17s\n",
      "Runtime of function \"fillna_revenue_new_items\": 10ms\n",
      "Runtime of function \"add_month_feature\": 1min & 0s\n",
      "Runtime of function \"add_days_in_month\": 58s\n",
      "Runtime of function \"add_item_last_sale_in_shop\": 1min & 21s\n",
      "Runtime of function \"add_item_last_sale\": 1min & 0s\n",
      "Runtime of function \"add_item_first_sale_in_shop\": 1min & 23s\n",
      "Runtime of function \"add_item_first_sale\": 1min & 0s\n",
      "Runtime of function \"apply_label_lag_feature\": 30s\n",
      "Runtime of function \"add_detailed_lag_features\": 9min & 21s\n",
      "Runtime of function \"add_average_price\": 8s\n",
      "Runtime of function \"add_average_price_during_month\": 3s\n",
      "Runtime of function \"add_lag_to_avg_price_during_month\": 49s\n",
      "Runtime of function \"add_price_trend\": 1s\n",
      "Runtime of function \"add_shop_revenue_per_month\": 3s\n",
      "Runtime of function \"add_shop_avg_revenue\": 3s\n",
      "Runtime of function \"add_shop_avg_revenue_trend\": 56ms\n",
      "Runtime of function \"add_shop_avg_revenue_trend_lag\": 31s\n",
      "Runtime of function \"fillna_lag_features\": 3s\n",
      "Runtime of function \"drop_month_col\": 2s\n",
      "Runtime of function \"fix_col_data_types\": 180ms\n"
     ]
    }
   ],
   "source": [
    "if run_fe:\n",
    "    feature_engineered = time_runtime(apply_cat_info, feature_engineered)\n",
    "    feature_engineered = time_runtime(apply_shop_duplicates, feature_engineered)\n",
    "    feature_engineered = time_runtime(apply_shop_location, feature_engineered)\n",
    "    feature_engineered = time_runtime(encode_categorical_data, feature_engineered)\n",
    "\n",
    "    feature_engineered = time_runtime(add_item_cat_avg_price, feature_engineered)\n",
    "    feature_engineered = time_runtime(apply_item_cat_avg_price_to_new_items, feature_engineered)\n",
    "    feature_engineered = time_runtime(apply_revenue_feature, feature_engineered)\n",
    "    feature_engineered = time_runtime(fillna_revenue_new_items, feature_engineered)\n",
    "\n",
    "    feature_engineered = time_runtime(add_month_feature, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_days_in_month, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_item_last_sale_in_shop, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_item_last_sale, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_item_first_sale_in_shop, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_item_first_sale, feature_engineered)\n",
    "\n",
    "    feature_engineered = time_runtime(apply_label_lag_feature, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_detailed_lag_features, feature_engineered)\n",
    "\n",
    "    feature_engineered = time_runtime(add_average_price, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_average_price_during_month, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_lag_to_avg_price_during_month, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_price_trend, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_shop_revenue_per_month, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_shop_avg_revenue, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_shop_avg_revenue_trend, feature_engineered)\n",
    "    feature_engineered = time_runtime(add_shop_avg_revenue_trend_lag, feature_engineered)\n",
    "\n",
    "    feature_engineered = time_runtime(fillna_lag_features, feature_engineered)\n",
    "    feature_engineered = time_runtime(drop_month_col, feature_engineered)\n",
    "    feature_engineered = time_runtime(fix_col_data_types, feature_engineered)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Serialize data to store it locally"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "if run_fe:\n",
    "    # sort it prior to storing\n",
    "    feature_engineered = feature_engineered.sort_values(['original_index'])\n",
    "    feature_engineered = feature_engineered[feature_engineered.date_block_num > 11]\n",
    "    feature_engineered = feature_engineered.reset_index(drop=True)\n",
    "    feature_engineered.drop('index', axis=1, inplace=True)\n",
    "    feature_engineered.drop('original_index', axis=1, inplace=True)\n",
    "    feature_engineered.to_pickle('data.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}